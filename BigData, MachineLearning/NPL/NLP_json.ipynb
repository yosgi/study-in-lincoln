{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83708667-4fdc-1563-7b3a-06b6575d2865",
        "id": "v6lD10h4vHgj"
      },
      "source": [
        "# Natural Language Processing - Examples\n",
        "\n",
        "There are many python packages for NLP out there, but we can cover the important bases once we master a handful of them.\n",
        "\n",
        "   * NLTK Book: https://www.nltk.org/    \n",
        "   * TextBlob : https://textblob.readthedocs.io/en/dev/index.html    \n",
        "   * Spacy : https://spacy.io/\n",
        "\n",
        "In additional to these there are few other libraries such as Gensim and Stanford’s CoreNLP that can be explored as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkvZ0fZqvHgl"
      },
      "source": [
        "<a id='1'></a>\n",
        "# 1. Load libraries and Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c1NfkEeMvHgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1767c9b5-c372-4c9c-b710-3fedec975646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.4\n",
            "  Downloading nltk-3.4.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.4) (1.16.0)\n",
            "Collecting singledispatch (from nltk==3.4)\n",
            "  Downloading singledispatch-4.1.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading singledispatch-4.1.0-py2.py3-none-any.whl (6.7 kB)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436383 sha256=8c0a6270b32c54d2d539d268fcd7ca93cc6893addc3b14fd1cd6df40323a1b16\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/cb/3e/ae17c28eb286abfcd886fdab69f9533bb060dc0a74f3b41d0d\n",
            "Successfully built nltk\n",
            "Installing collected packages: singledispatch, nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.4 singledispatch-4.1.0\n",
            "Collecting textblob==0.15.3\n",
            "  Downloading textblob-0.15.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob==0.15.3) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.15.3) (1.16.0)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.15.3) (4.1.0)\n",
            "Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.5/636.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.17.1\n",
            "    Uninstalling textblob-0.17.1:\n",
            "      Successfully uninstalled textblob-0.17.1\n",
            "Successfully installed textblob-0.15.3\n",
            "Collecting gensim==3.8.2\n",
            "  Downloading gensim-3.8.2.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.2) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.2) (1.16.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.2) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart_open>=1.8.1->gensim==3.8.2) (1.16.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-3.8.2-cp310-cp310-linux_x86_64.whl size=24677393 sha256=dcf2f95a0edc6a8206e1ae00acccc5329876e917016521d61256e767a3b312a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/03/36/82090cfd4d91edcffcddb72ce379c0db1b8c3e257342dc223c\n",
            "Successfully built gensim\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed gensim-3.8.2\n",
            "Collecting SpaCy==2.2.0\n",
            "  Downloading spacy-2.2.0.tar.gz (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.4\n",
        "!pip install textblob==0.15.3\n",
        "!pip install gensim==3.8.2\n",
        "!pip install -U SpaCy==2.2.0\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jqnikkLAvHgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf75e1bf-ea49-46e3-9513-7042e8f9c482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "#Run the command python -m spacy download en_core_web_sm to download this\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "#Other helper packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Download nltk data lobraries. All can be downloaded by using nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NyKmEQ1kvHgo"
      },
      "outputs": [],
      "source": [
        "#Diable the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Sbs3r8vHgp"
      },
      "source": [
        "<a id='2'></a>\n",
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbenHGG5vHgp"
      },
      "source": [
        "<a id='2.1'></a>\n",
        "## 2.1. Tokenization\n",
        "Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "5d8fee34-f454-2642-8b06-ed719f0317e1",
        "id": "V7wevGxnvHgp"
      },
      "outputs": [],
      "source": [
        "#Text to tokenize\n",
        "text = \"This is a tokenize test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJxjYu-JvHgq"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvqEkwLGvHgq"
      },
      "source": [
        "The NLTK data package includes a pre-trained Punkt tokenizer for English, which has alreayd been loaded before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Oa9ZYKB1vHgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cdbd5b-4d91-4177-b250-92fa5359dc22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'tokenize', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF_OOLTCvHgr"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKy1vFCwvHgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d097347-0702-4d1b-bb58-f2a1bb2cfa80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['This', 'is', 'a', 'tokenize', 'test'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "TextBlob(text).words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ89daBbvHgr"
      },
      "source": [
        "<a id='2.2'></a>\n",
        "## 2.2. Stop Words Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xot8FCwfvHgs"
      },
      "source": [
        "Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words. The code for removing stop words using SpaCy library is shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHIWcZaJvHgs"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1zRS_qivHgs"
      },
      "source": [
        "We first load the language model and store it in the stop_words variable. The stopwords.words('english') is a set of default stop words for English language model in NLTK. Next, we simply iterate through each word in the input text and if the word exists in the stop word set of the NLTK language model, the word is removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wa3TpK5NvHgs"
      },
      "outputs": [],
      "source": [
        "text = \"S&P and NASDAQ are the two most popular indices in US\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YEMIQEuBvHgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83de6403-a2aa-48e9-e68c-931e0e6639ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S', '&', 'P', 'NASDAQ', 'two', 'popular', 'indices', 'US']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw= [word for word in text_tokens if not word in stop_words]\n",
        "\n",
        "print(tokens_without_sw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayd-h7-zvHgt"
      },
      "source": [
        "As we can see some of the stop words such as \"are\", \"of\", \"most\" etc are removed from the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggTVtRpnvHgt"
      },
      "source": [
        "<a id='2.3'></a>\n",
        "## 2.3. Stemming\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0mpwbN8xvHgt"
      },
      "outputs": [],
      "source": [
        "text = \"It's a Stemming testing\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYZXkaDYvHgt"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ovl71_XKvHgt"
      },
      "outputs": [],
      "source": [
        "parsed_text = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IYakQqkwvHgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771fba36-4d15-47ca-a046-595ed6518972"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stemming', 'stem'), ('testing', 'test')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Initialize stemmer.\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Stem each word.\n",
        "[(word, stemmer.stem(word)) for i, word in enumerate(parsed_text)\n",
        " if word.lower() != stemmer.stem(parsed_text[i])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5A8S9dWvHgu"
      },
      "source": [
        "<a id='2.4'></a>\n",
        "## 2.4. Lemmetization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJo4WV2rvHgu"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MKvFvfZSvHgu"
      },
      "outputs": [],
      "source": [
        "text = \"This world has a lot of faces \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "otKkxNdPvHgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445cb3df-984b-4b41-f57b-efd905af82a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['This', 'world', 'has', 'a', 'lot', 'of', 'faces'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from textblob import Word\n",
        "parsed_data= TextBlob(text).words\n",
        "parsed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "29aViF1XvHgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1363b8-1073-4359-c6da-a69fdcd73cf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('has', 'ha'), ('faces', 'face')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "[(word, word.lemmatize()) for i, word in enumerate(parsed_data)\n",
        " if word != parsed_data[i].lemmatize()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkkkW4eLvHgv"
      },
      "source": [
        "<a id='2.5'></a>\n",
        "## 2.5. POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GcwyUhgvHgv"
      },
      "source": [
        "Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mjHNNDynvHgw"
      },
      "outputs": [],
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWvetawMvHgw"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6BMkETMIvHgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328bc2ce-2ff8-43df-c3bb-81acb350c59e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Google', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('looking', 'VBG'),\n",
              " ('at', 'IN'),\n",
              " ('buying', 'VBG'),\n",
              " ('U.K.', 'NNP'),\n",
              " ('startup', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('1', 'CD'),\n",
              " ('billion', 'CD')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "TextBlob(text).tags"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a list of tokens with their corresponding Part-of-Speech (POS) tags.\n",
        "\n",
        "('Google', 'NNP'): Google is a proper noun.\n",
        "\n",
        "('is', 'VBZ'): is is a verb, third person singular present.\n",
        "\n",
        "('looking', 'VBG'): looking is a verb, gerund or present participle.\n",
        "\n",
        "('at', 'IN'): at is a preposition."
      ],
      "metadata": {
        "id": "9M7dRR04NiRY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pviES3dXvHgx"
      },
      "source": [
        "## Spacy- doing all at ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UXH8bCzvvHgx"
      },
      "outputs": [],
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-ZRxtyWAvHgx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "f8fac494-a161-4a5d-8ed1-6c3bee4e294e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Token  is_stop_word    lemma    POS\n",
              "0    Google         False   Google  PROPN\n",
              "1        is          True       be    AUX\n",
              "2   looking         False     look   VERB\n",
              "3        at          True       at    ADP\n",
              "4    buying         False      buy   VERB\n",
              "5      U.K.         False     U.K.  PROPN\n",
              "6   startup         False  startup   NOUN\n",
              "7       for          True      for    ADP\n",
              "8         $         False        $    SYM\n",
              "9         1         False        1    NUM\n",
              "10  billion         False  billion    NUM"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-741ba258-31e7-4ea6-9e2e-8a17e13aec12\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>is_stop_word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>POS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Google</td>\n",
              "      <td>False</td>\n",
              "      <td>Google</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>True</td>\n",
              "      <td>be</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>looking</td>\n",
              "      <td>False</td>\n",
              "      <td>look</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>True</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buying</td>\n",
              "      <td>False</td>\n",
              "      <td>buy</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U.K.</td>\n",
              "      <td>False</td>\n",
              "      <td>U.K.</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>startup</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>for</td>\n",
              "      <td>True</td>\n",
              "      <td>for</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>$</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>billion</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-741ba258-31e7-4ea6-9e2e-8a17e13aec12')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-741ba258-31e7-4ea6-9e2e-8a17e13aec12 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-741ba258-31e7-4ea6-9e2e-8a17e13aec12');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f5a26ff4-2361-4733-8822-8ed639064715\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f5a26ff4-2361-4733-8822-8ed639064715')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f5a26ff4-2361-4733-8822-8ed639064715 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"             columns=['Token', 'is_stop_word','lemma', 'POS'])\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"Token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"U.K.\",\n          \"Google\",\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_stop_word\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"U.K.\",\n          \"Google\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POS\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"PROPN\",\n          \"AUX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "pd.DataFrame([[t.text, t.is_stop, t.lemma_, t.pos_]\n",
        "              for t in doc],\n",
        "             columns=['Token', 'is_stop_word','lemma', 'POS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuLgDeZ1vHgy"
      },
      "source": [
        "<a id='2.6'></a>\n",
        "## 2.6. Name Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TFEyMJZevHgy"
      },
      "outputs": [],
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTX8yqJ0vHgz"
      },
      "source": [
        "### SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2BxyBd-bvHgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410fc4f6-e98b-442a-a6a5-7be69dcbe89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity:  Google\n",
            "Entity Type: ORG | Companies, agencies, institutions, etc.\n",
            "--\n",
            "Entity:  U.K.\n",
            "Entity Type: GPE | Countries, cities, states\n",
            "--\n",
            "Entity:  $1 billion\n",
            "Entity Type: MONEY | Monetary values, including unit\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "for entity in nlp(text).ents:\n",
        "    print(\"Entity: \", entity.text)\n",
        "    print(\"Entity Type: %s | %s\" % (entity.label_, spacy.explain(entity.label_)))\n",
        "    print(\"--\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EaWJXDIzvHgz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9ad5a1e7-bdf8-4c69-e155-cbc581eb381b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(nlp(text), style=\"ent\", jupyter = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "df6a4523-b385-69ee-c933-592826d81431",
        "id": "R4w_1eOCvHg0"
      },
      "source": [
        "<a id='3'></a>\n",
        "# 3. Feature Representation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcIWXEOovHg0"
      },
      "source": [
        "<a id='3.1'></a>\n",
        "## 3.1. Bag of Words - Word Count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV2UTVX5vHg0"
      },
      "source": [
        "In natural language processing, a common technique for extracting features from text is to place all of the words that occur in the text in a bucket. This aproach is called a bag of words model or BoW for short. It’s referred to as a “bag” of words because any information about the structure of the sentence is lost.The CountVectorizer from sklearn provides a simple way to both tokenize a collection of text documents and encode new documents using that vocabulary.The fit_transform\n",
        "function learns the vocabulary from one or more documents and encodes each document in the word as a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PCI0FKQdvHg0"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'We really love FINC612', 'This semester is over'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "42swB-4RvHg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618e75ca-4f27-4015-f0a4-3652315b6968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 0 1 0 1 0 1 1 0 1 0 0 1 2 0 1 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0]]\n",
            "{'the': 14, 'stock': 13, 'price': 10, 'of': 7, 'google': 3, 'jumps': 5, 'on': 8, 'earning': 1, 'data': 0, 'today': 16, 'we': 17, 'really': 11, 'love': 6, 'finc612': 2, 'this': 15, 'semester': 12, 'is': 4, 'over': 9}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "print( vectorizer.fit_transform(sentences).todense() )\n",
        "print( vectorizer.vocabulary_ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrpRVpBSvHg1"
      },
      "source": [
        "<a id='3.2'></a>\n",
        "## 3.2. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnvNdIOEvHg1"
      },
      "source": [
        "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
        "\n",
        "* Term Frequency: This summarizes how often a given word appears within a document.\n",
        "* Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
        "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
        "\n",
        "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "TFIDF = vectorizer.fit_transform(sentences)\n",
        "print(vectorizer.get_feature_names_out()[-10:]) # Use get_feature_names_out() instead of get_feature_names()\n",
        "print(TFIDF.shape)\n",
        "print(TFIDF.toarray())"
      ],
      "metadata": {
        "id": "xnfdkkdqPKwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c3b88e-e718-4761-f847-76b60bf908aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['earning' 'finc612' 'google' 'jumps' 'love' 'price' 'really' 'semester'\n",
            " 'stock' 'today']\n",
            "(3, 11)\n",
            "[[0.37796447 0.37796447 0.         0.37796447 0.37796447 0.\n",
            "  0.37796447 0.         0.         0.37796447 0.37796447]\n",
            " [0.         0.         0.57735027 0.         0.         0.57735027\n",
            "  0.         0.57735027 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7uw_3k1vHg6"
      },
      "source": [
        "<a id='5'></a>\n",
        "# 5 NLP Recipies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BscphbOWvHg6"
      },
      "source": [
        "<a id='5.1'></a>\n",
        "## 5.1. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdnNl9R6vHg7"
      },
      "source": [
        "Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping us understand the sentiments behind a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vuOhDtlvHg7"
      },
      "source": [
        "With the help of Sentiment Analysis using Textblob the sentiment analysis can be performed in few lines of code. TextBlob provides polarity and subjectivity estimates for parsed documents using dictionaries provided by the Pattern library. The polarity defines the phase of emotions expressed in the analyzed sentence. Polarity alone is not enough to deal with complex text sentences. Subjectivity helps in determining personal states of the speaker including Emotions, Beliefs and opinions. It has values from 0 to 1 and a value closer to 0 shows the sentence is objective and vice versa.\n",
        "\n",
        "The texblob sentiment function is pretrained and map adjectives frequently found in movie reviews(source code: https://textblob.readthedocs.io/en/dev/_modules/textblob/en/sentiments.html) to sentiment polarity scores, ranging from -1 to +1 (negative ↔ positive) and a similar subjectivity score (objective ↔ subjective).\n",
        "\n",
        "The .sentiment attribute provides the average for each over the relevant tokens, whereas the .sentiment_assessments attribute lists the underlying values for each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "T9jv5nqDvHg7"
      },
      "outputs": [],
      "source": [
        "text1 = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, touching their highest level in 14 months, after the U.S. government said a $25M glyphosate decision against the company should be reversed.\"\n",
        "text2 = \"Apple declares poor in revenues\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "50KXP8T7vHg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aef17a7-d02f-4fdb-a307-df05197dcde2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "TextBlob(text1).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HO9jFNYUvHg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d36cce3-8a90-40f7-d71b-7b81dd2f9e6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.5, subjectivity=0.5, assessments=[(['touching'], 0.5, 0.5, None)])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "TextBlob(text1).sentiment_assessments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QoK_21NLvHg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e928d0e6-b1b9-4ea7-a03a-4534c7d70892"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.4"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "TextBlob(text2).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yyIZsEx5vHg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016020f7-034f-41b3-dd42-510883f52197"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.4, subjectivity=0.6, assessments=[(['poor'], -0.4, 0.6, None)])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "TextBlob(text2).sentiment_assessments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p50iuz-PvHg8"
      },
      "source": [
        "<a id='5.2'></a>\n",
        "## 5.2. Text Similarity\n",
        "Finding similarity between text is at the heart of almost all text mining methods, for example, text classification, clustering, recommendation, and many more. In order to calculate similarity between two text snippets, the usual way is to convert the text into its corresponding vector representation, for which there are many methods like word embedding of text, and then calculate similarity or difference using different distance metrics such as cosine-similarity and euclidean distance applicable to vectors. The underlying vector representations come from a word embedding model which generally produces a dense multi-dimensional semantic representation of words (as shown in the example). Using this vector representation, we can calculate similarities and dissimilarities between tokens, named entities, noun phrases, sentences and documents. The example below shows how to calculate similarities between two documents and tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X60-57C1vHg8"
      },
      "outputs": [],
      "source": [
        "text1 = \"Barack Obama was the 44th president of the United States of America.\"\n",
        "text2 = \"Donald Trump is the 45th president of the United States of America.\"\n",
        "text3 = \"SpaCy and NLTK are two popular NLP libraries in Python community.\"\n",
        "doc1 = nlp(text1); doc2 = nlp(text2); doc3 = nlp(text3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KZTR3VlvvHg9"
      },
      "outputs": [],
      "source": [
        "def text_similarity(inp_obj1, inp_obj2):\n",
        "    return inp_obj1.similarity(inp_obj2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uYqBy99AvHg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ece0c16-d9a2-4bf3-cb03-94cbab320eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between doc1 and doc2:  0.9745025257196641\n",
            "Similarity between doc1 and doc3:  0.6599942528535854\n"
          ]
        }
      ],
      "source": [
        "print(\"Similarity between doc1 and doc2: \", text_similarity(doc1, doc2))\n",
        "print(\"Similarity between doc1 and doc3: \", text_similarity(doc1, doc3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JqTKd90SvHg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8b946c-4f2e-47ab-d682-b1cc3438a194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 1: Apple, Token 2: Apple - Similarity: 1.000000\n",
            "Token 1: Apple, Token 2: orange - Similarity: 0.184876\n",
            "Token 1: Apple, Token 2: cats - Similarity: -0.047864\n",
            "Token 1: orange, Token 2: Apple - Similarity: 0.184876\n",
            "Token 1: orange, Token 2: orange - Similarity: 1.000000\n",
            "Token 1: orange, Token 2: cats - Similarity: 0.139896\n",
            "Token 1: cats, Token 2: Apple - Similarity: -0.047864\n",
            "Token 1: cats, Token 2: orange - Similarity: 0.139896\n",
            "Token 1: cats, Token 2: cats - Similarity: 1.000000\n"
          ]
        }
      ],
      "source": [
        "def token_similarity(doc):\n",
        "    for token1 in doc:\n",
        "        for token2 in doc:\n",
        "            print(\"Token 1: %s, Token 2: %s - Similarity: %f\" % (token1.text, token2.text, token1.similarity(token2)))\n",
        "\n",
        "doc4 = nlp(\"Apple orange cats\")\n",
        "token_similarity(doc4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vPP0gXLqZc5b"
      }
    }
  ],
  "metadata": {
    "_change_revision": 206,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YkvZ0fZqvHgl",
        "YbenHGG5vHgp",
        "gQ89daBbvHgr",
        "QHIWcZaJvHgs",
        "ggTVtRpnvHgt",
        "aYZXkaDYvHgt",
        "R5A8S9dWvHgu",
        "HJo4WV2rvHgu",
        "JkkkW4eLvHgv",
        "AWvetawMvHgw",
        "pviES3dXvHgx",
        "vuLgDeZ1vHgy",
        "rTX8yqJ0vHgz",
        "R4w_1eOCvHg0",
        "fcIWXEOovHg0",
        "yrpRVpBSvHg1",
        "A7uw_3k1vHg6",
        "BscphbOWvHg6",
        "p50iuz-PvHg8"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}